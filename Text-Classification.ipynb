{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Text Classifier #\n",
    "Author: Christin Seifert, licensed under the Creative Commons Attribution 3.0 Unported License https://creativecommons.org/licenses/by/3.0/ \n",
    "It is based on a tutorial of Nils Witt (https://github.com/n-witt/MachineLearningWithText_SS2017)\n",
    "\n",
    "\n",
    "This is a tutorial for learning and evaluating a simple naive bayes classifier on for a simple text classification problem. In this tutorial you will:\n",
    "\n",
    "* inspect the data you will be using to train the decision tree \n",
    "* train a decision tree \n",
    "* evaluate how well the decision tree does \n",
    "* visualize the decision tree\n",
    "\n",
    "It is assumed that you have some general knowledge on \n",
    "* document-term matrices\n",
    "* what a Naive Bayes classifier does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting texts to features\n",
    "\n",
    "We wil start with a small example of 3 SMS'. The texts in the SMS are the following \"call me tonight\", \"Call me a cab\", \"please call me... PLEASE!\" In order to do text classification we need to convert the text into a feature vector. We will follow a very simple approach here:\n",
    "1. Find out which different words (or tokens) are used in the text. These makes up the vocabulary.\n",
    "2. The length of a vector for each document then is the size of the vocabulary, and each entry in the vector corresponds to one word. This means, the first entry in the vector corresponds to the first word in the vocabulary, the second to the second and .. you get the logic ;-)\n",
    "3. For each document we simply cound how often each word occurs and write it at the index in the vector that corresponds to this word. \n",
    "\n",
    "All those things can easily be done with the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vect = CountVectorizer()\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data \n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'tonight']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that all words are lower case now? And that we ignored punctuation? Whether this is a good idea, depends on the application. E.g. for detecting emotions in texts, smilies (punctutation) might be a helpful feature. But for now, let's keep it simple.\n",
    "\n",
    "Now we generate a document-term matrix. In this matrix each row corresponds to one document, each column to one feature. Entry `(i,j)` tells us how often word `j` occurs in document `i`.\n",
    "\n",
    "_Note:_ The \"how often\" is only true if we use the count vectorizer. Instead of word count there are many other possible features.\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-232957a343fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# transform training data into a 'document-term matrix'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msimple_train_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msimple_train_dtm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                 \"string object received.\")\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a pandas data frame to store the vector and the feature names together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in general this is an aweful lot of zeros (think of how many of all English words are present in a SMS), the more efficient way to store the information is as a sparse matrix. For humans this is a bit harder to read.\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "type(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the feature vector for a previously unseen text\n",
    "In order to make predictions for unseen data, the new observation must have the same features as the training observations, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me, I don't like you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>tonight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  tonight\n",
       "0    0        0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple spam filter \n",
    "Now we are going to implement a simple spam filter for SMS messages. We are given a data set with SMS that are already annotated with either spam or ham (=not spam). We first load the data set and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'material/sms.tsv'\n",
    "sms = pd.read_table(path, header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 10 rows\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the label to a numerical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...          1\n",
       "6   ham  Even my brother is not like to speak with me. ...          0\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...          0\n",
       "8  spam  WINNER!! As a valued network customer you have...          1\n",
       "9  spam  Had your mobile 11 months or more? U R entitle...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the conversion worked\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our text in the column `message` and our label in the column `label_num`. Let's have a look at the sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.message\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at the text of the first 5 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare the data for the classifier. First split it into a training and a test set. There is a convenient method `train_test_split` available that helps us with that. We use a fixed random state `random_state=42`to split randomly, but at the same time get the same results each time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b9a55d629462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# split X and y into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the data preprocessing knowledge from above and generate the vocabulary. We will do this ONLY on the training data set, because we presume to have no knowledge whatsoever about the test data set. So we don't know the test data's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1517e7a7a96f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# learn training data vocabulary, then use it to create a document-term matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect = CountVectorizer(stop_words='english',ngram_range=(1, 2), min_df=2,max_df=0.5)\n",
    "print(X_train[:5])\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x6920 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform the test data set using the same vocabulary (that is using the same `vect` object that internally knows the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x6920 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10621 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are at the stage where we have a matrix of features and the corresponding labels. We can now train a classifier for spam detection on sms. We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_dtm, y_train)\n",
    "y_test_pred = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877961234745154"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1201,    6],\n",
       "       [  11,  175]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Spaminess\" of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start: the estimator has several fields that allow us to examine its internal state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 6681,\n",
       " 'valued': 6375,\n",
       " 'network': 4166,\n",
       " 'customer': 1629,\n",
       " 'selected': 5188,\n",
       " '900': 553,\n",
       " 'prize': 4719,\n",
       " 'reward': 5001,\n",
       " 'claim': 1331,\n",
       " 'code': 1377,\n",
       " 'valid': 6368,\n",
       " '12': 182,\n",
       " 'hours': 2931,\n",
       " 'winner valued': 6684,\n",
       " 'valued network': 6379,\n",
       " 'network customer': 4167,\n",
       " 'claim code': 1335,\n",
       " 'valid 12': 6369,\n",
       " 'hope': 2902,\n",
       " 'showing': 5316,\n",
       " 'care': 1195,\n",
       " 'live': 3489,\n",
       " 'dream': 1935,\n",
       " 'derek': 1776,\n",
       " 'class': 1345,\n",
       " 'aight': 658,\n",
       " 'lemme': 3400,\n",
       " 'know': 3271,\n",
       " 'lemme know': 3401,\n",
       " 'yo': 6898,\n",
       " 'watching': 6557,\n",
       " 'movie': 4040,\n",
       " 'watching movie': 6559,\n",
       " 'don': 1880,\n",
       " 'tell': 5809,\n",
       " 'friend': 2359,\n",
       " 'sure': 5718,\n",
       " 'want': 6508,\n",
       " 'smokes': 5435,\n",
       " 'spend': 5546,\n",
       " 'begging': 914,\n",
       " 'come': 1417,\n",
       " 'smoke': 5433,\n",
       " 'don tell': 1894,\n",
       " 'tell friend': 5813,\n",
       " 'want live': 6517,\n",
       " 'pain': 4422,\n",
       " 'better': 937,\n",
       " 'shall': 5272,\n",
       " 'send': 5201,\n",
       " 'mail': 3721,\n",
       " 'id': 2982,\n",
       " 'mail id': 3722,\n",
       " 'ok': 4325,\n",
       " 'just': 3177,\n",
       " 'couple': 1560,\n",
       " 'days': 1713,\n",
       " 'lt': 3666,\n",
       " 'couple days': 1561,\n",
       " 'awake': 824,\n",
       " 'snow': 5454,\n",
       " 'sounds': 5514,\n",
       " 'like': 3437,\n",
       " 'plan': 4561,\n",
       " 'cardiff': 1194,\n",
       " 'cold': 1385,\n",
       " 'sitting': 5370,\n",
       " 'sounds like': 5517,\n",
       " 'thanks': 5883,\n",
       " 've': 6385,\n",
       " 'lovely': 3637,\n",
       " 'rock': 5031,\n",
       " 'cancer': 1183,\n",
       " 'moms': 3997,\n",
       " 'making': 3747,\n",
       " 'big': 944,\n",
       " 'deal': 1717,\n",
       " 'regular': 4918,\n",
       " 'aka': 670,\n",
       " 'pap': 4428,\n",
       " 'big deal': 946,\n",
       " 'tones': 6076,\n",
       " '150p': 204,\n",
       " 'reply': 4946,\n",
       " 'poly': 4637,\n",
       " 'mono': 4004,\n",
       " 'cha': 1253,\n",
       " 'yeah': 6859,\n",
       " 'slow': 5407,\n",
       " 'stop': 5617,\n",
       " 'txt': 6173,\n",
       " 'thought': 5934,\n",
       " 'box': 1026,\n",
       " 'hey': 2819,\n",
       " 'late': 3336,\n",
       " 'need': 4142,\n",
       " 'drink': 1942,\n",
       " 'tea': 5789,\n",
       " 'coffee': 1382,\n",
       " 'hey late': 2824,\n",
       " 'tea coffee': 5790,\n",
       " 'yes': 6883,\n",
       " 'place': 4555,\n",
       " 'town': 6107,\n",
       " 'meet': 3815,\n",
       " 'exciting': 2106,\n",
       " 'adult': 627,\n",
       " 'singles': 5356,\n",
       " 'uk': 6218,\n",
       " 'chat': 1280,\n",
       " '86688': 522,\n",
       " 'msg': 4054,\n",
       " 'txt chat': 6185,\n",
       " 'chat 86688': 1281,\n",
       " '86688 150p': 523,\n",
       " '150p msg': 207,\n",
       " 'tired': 5991,\n",
       " 'haven': 2764,\n",
       " 'slept': 5401,\n",
       " 'past': 4456,\n",
       " 'nights': 4221,\n",
       " 'thank': 5880,\n",
       " 'dear': 1719,\n",
       " 'recharge': 4896,\n",
       " 'rakhesh': 4813,\n",
       " 'business': 1103,\n",
       " 'knackered': 3268,\n",
       " 'came': 1162,\n",
       " 'home': 2882,\n",
       " 'went': 6619,\n",
       " 'sleep': 5391,\n",
       " 'good': 2539,\n",
       " 'time': 5964,\n",
       " 'work': 6761,\n",
       " 'came home': 1163,\n",
       " 'sleep good': 5393,\n",
       " 'good time': 2557,\n",
       " 'urgent': 6332,\n",
       " 'mobile': 3945,\n",
       " 'number': 4274,\n",
       " 'awarded': 834,\n",
       " '2000': 259,\n",
       " 'guaranteed': 2673,\n",
       " 'land': 3320,\n",
       " 'line': 3462,\n",
       " '3030': 333,\n",
       " '12hrs': 188,\n",
       " 'urgent mobile': 6337,\n",
       " 'mobile number': 3962,\n",
       " 'number awarded': 4276,\n",
       " 'awarded 2000': 837,\n",
       " '2000 prize': 262,\n",
       " 'prize guaranteed': 4724,\n",
       " 'land line': 3321,\n",
       " 'line claim': 3464,\n",
       " 'claim 3030': 1333,\n",
       " '3030 valid': 334,\n",
       " 'valid 12hrs': 6370,\n",
       " 'pple': 4682,\n",
       " '700': 461,\n",
       " 'excellent': 2105,\n",
       " 'location': 3536,\n",
       " 'wif': 6646,\n",
       " 'noe': 4228,\n",
       " 'wat': 6545,\n",
       " 'dat': 1674,\n",
       " 'sells': 5197,\n",
       " '4d': 385,\n",
       " 'closes': 1365,\n",
       " 'wat time': 6550,\n",
       " 'time place': 5976,\n",
       " 'free': 2301,\n",
       " 'entry': 2056,\n",
       " 'wkly': 6716,\n",
       " 'comp': 1452,\n",
       " 'chance': 1256,\n",
       " 'win': 6661,\n",
       " 'latest': 3347,\n",
       " 'nokia': 4232,\n",
       " '250': 279,\n",
       " 'cash': 1220,\n",
       " 'wk': 6709,\n",
       " 'great': 2614,\n",
       " '80878': 494,\n",
       " 'http': 2946,\n",
       " 'www': 6807,\n",
       " 'com': 1407,\n",
       " '08715705022': 94,\n",
       " 'free entry': 2317,\n",
       " 'wkly comp': 6717,\n",
       " 'comp chance': 1453,\n",
       " 'chance win': 1261,\n",
       " 'latest nokia': 3352,\n",
       " '250 cash': 281,\n",
       " 'cash wk': 1229,\n",
       " 'wk txt': 6711,\n",
       " 'http www': 2948,\n",
       " 'appointment': 739,\n",
       " 'camera': 1165,\n",
       " 'sipix': 5357,\n",
       " 'digital': 1817,\n",
       " '09061221066': 120,\n",
       " 'fromm': 2389,\n",
       " 'landline': 3322,\n",
       " 'delivery': 1765,\n",
       " '28': 291,\n",
       " 'camera awarded': 1168,\n",
       " 'awarded sipix': 841,\n",
       " 'sipix digital': 5358,\n",
       " 'digital camera': 1818,\n",
       " 'camera 09061221066': 1167,\n",
       " '09061221066 fromm': 121,\n",
       " 'fromm landline': 2390,\n",
       " 'landline delivery': 3325,\n",
       " 'delivery 28': 1766,\n",
       " '28 days': 292,\n",
       " 'looking': 3564,\n",
       " 'yup': 6913,\n",
       " 'leh': 3395,\n",
       " 'probably': 4731,\n",
       " 'gotta': 2598,\n",
       " 'check': 1292,\n",
       " 'yeah probably': 6865,\n",
       " 'nope': 4253,\n",
       " 'juz': 3216,\n",
       " 'oh': 4313,\n",
       " 'fuck': 2391,\n",
       " 'bed': 904,\n",
       " 'wid': 6640,\n",
       " '25': 277,\n",
       " 'year': 6868,\n",
       " 'old': 4355,\n",
       " 'giv': 2491,\n",
       " 'da': 1647,\n",
       " 'gossip': 2572,\n",
       " 'l8r': 3310,\n",
       " 'xxx': 6837,\n",
       " 'oh fuck': 4316,\n",
       " 'year old': 6871,\n",
       " 'usual': 6359,\n",
       " 'iam': 2969,\n",
       " 'fine': 2227,\n",
       " 'happy': 2744,\n",
       " 'amp': 687,\n",
       " 'doing': 1872,\n",
       " 'usual iam': 6360,\n",
       " 'iam fine': 2971,\n",
       " 'fine happy': 2229,\n",
       " 'happy amp': 2745,\n",
       " 'amp doing': 690,\n",
       " 'got': 2573,\n",
       " 'shitload': 5294,\n",
       " 'girl': 2482,\n",
       " 'world': 6772,\n",
       " 'gud': 2680,\n",
       " 'mrng': 4050,\n",
       " 'hav': 2762,\n",
       " 'nice': 4205,\n",
       " 'day': 1685,\n",
       " 'gud mrng': 2682,\n",
       " 'mrng dear': 4051,\n",
       " 'dear hav': 1725,\n",
       " 'hav nice': 2763,\n",
       " 'nice day': 4206,\n",
       " 'did': 1792,\n",
       " 'chechi': 1291,\n",
       " 'drug': 1954,\n",
       " 'anymore': 722,\n",
       " 'wasn': 6541,\n",
       " 'paying': 4461,\n",
       " 'morning': 4022,\n",
       " 'love': 3607,\n",
       " 'wish': 6692,\n",
       " 'feeling': 2196,\n",
       " 'opportunity': 4374,\n",
       " 'babe': 852,\n",
       " 'kiss': 3264,\n",
       " 'good morning': 2551,\n",
       " 'morning love': 4025,\n",
       " 'wish great': 6695,\n",
       " 'great day': 2617,\n",
       " 'feeling better': 2197,\n",
       " 'babe love': 855,\n",
       " 'haha': 2706,\n",
       " 'awesome': 845,\n",
       " 'doin': 1870,\n",
       " 'tonight': 6083,\n",
       " 'haha awesome': 2707,\n",
       " 'doin tonight': 1871,\n",
       " 'talk': 5764,\n",
       " 'pa': 4414,\n",
       " 'able': 566,\n",
       " 'dont': 1898,\n",
       " 'dont know': 1903,\n",
       " 'major': 3729,\n",
       " 'community': 1451,\n",
       " 'mel': 3832,\n",
       " 'money': 4000,\n",
       " 'mate': 3779,\n",
       " 'posted': 4661,\n",
       " 'sounds good': 5516,\n",
       " 'anyways': 726,\n",
       " 'gym': 2700,\n",
       " 'smiles': 5428,\n",
       " 'having': 2769,\n",
       " 'miss': 3912,\n",
       " 'hope ok': 2911,\n",
       " 'having good': 2772,\n",
       " 'good day': 2544,\n",
       " 'day babe': 1690,\n",
       " 'babe miss': 856,\n",
       " 'mystery': 4106,\n",
       " 'solved': 5465,\n",
       " 'email': 2018,\n",
       " 'sent': 5231,\n",
       " 'isn': 3096,\n",
       " 'sweetie': 5743,\n",
       " 'email sent': 2020,\n",
       " 'hello': 2803,\n",
       " 'lover': 3641,\n",
       " 'goes': 2511,\n",
       " 'new': 4175,\n",
       " 'job': 3143,\n",
       " 'think': 5907,\n",
       " 'wake': 6482,\n",
       " 'slave': 5389,\n",
       " 'teasing': 5799,\n",
       " 'sea': 5163,\n",
       " 'new job': 4178,\n",
       " 'send teasing': 5223,\n",
       " 'teasing kiss': 5800,\n",
       " 'kiss sea': 3266,\n",
       " 'trying': 6144,\n",
       " 'contact': 1498,\n",
       " 'dating': 1682,\n",
       " 'service': 5245,\n",
       " 'trying contact': 6145,\n",
       " 'dating service': 1683,\n",
       " 'mobile landline': 3960,\n",
       " 'll': 3498,\n",
       " 'rcv': 4832,\n",
       " 'msgs': 4074,\n",
       " 'hardcore': 2755,\n",
       " 'services': 5251,\n",
       " 'text': 5844,\n",
       " 'age': 644,\n",
       " 'verify': 6402,\n",
       " 'yr': 6904,\n",
       " 'try': 6140,\n",
       " 'ya': 6841,\n",
       " 'display': 1846,\n",
       " 'subs': 5672,\n",
       " 'todays': 6035,\n",
       " 'voda': 6433,\n",
       " 'numbers': 4281,\n",
       " 'ending': 2029,\n",
       " 'receive': 4885,\n",
       " 'match': 3775,\n",
       " '08712300220': 80,\n",
       " 'quoting': 4798,\n",
       " '3100': 340,\n",
       " 'standard': 5577,\n",
       " 'rates': 4827,\n",
       " 'app': 732,\n",
       " 'todays voda': 6038,\n",
       " 'voda numbers': 6434,\n",
       " 'numbers ending': 4282,\n",
       " 'selected receive': 5189,\n",
       " 'match 08712300220': 3776,\n",
       " '08712300220 quoting': 81,\n",
       " 'quoting claim': 4799,\n",
       " 'code 3100': 1378,\n",
       " '3100 standard': 341,\n",
       " 'standard rates': 5578,\n",
       " 'rates app': 4828,\n",
       " 'driving': 1948,\n",
       " 'raining': 4810,\n",
       " 'caught': 1236,\n",
       " 'mrt': 4052,\n",
       " 'station': 5597,\n",
       " 'lor': 3570,\n",
       " 'mrt station': 4053,\n",
       " 'midnight': 3872,\n",
       " 'yeah don': 6861,\n",
       " 'ready': 4851,\n",
       " 'moan': 3938,\n",
       " 'scream': 5157,\n",
       " 'ready moan': 4853,\n",
       " 'moan scream': 3939,\n",
       " 'senthil': 5239,\n",
       " 'hsbc': 2945,\n",
       " 'senthil hsbc': 5240,\n",
       " 'orange': 4381,\n",
       " 'phone': 4506,\n",
       " 'upgrade': 6272,\n",
       " 'loyalty': 3658,\n",
       " '0207': 12,\n",
       " '153': 220,\n",
       " 'offer': 4298,\n",
       " 'ends': 2030,\n",
       " '26th': 289,\n",
       " 'july': 3173,\n",
       " 'apply': 734,\n",
       " 'opt': 4375,\n",
       " 'available': 814,\n",
       " 'orange customer': 4383,\n",
       " 'customer claim': 1630,\n",
       " 'claim free': 1337,\n",
       " 'free camera': 2313,\n",
       " 'camera phone': 1172,\n",
       " 'phone upgrade': 4519,\n",
       " 'upgrade loyalty': 6273,\n",
       " 'loyalty 0207': 3659,\n",
       " '0207 153': 13,\n",
       " 'offer ends': 4300,\n",
       " '26th july': 290,\n",
       " 'apply opt': 737,\n",
       " 'ur': 6278,\n",
       " '150': 198,\n",
       " 'worth': 6784,\n",
       " 'discount': 1841,\n",
       " 'vouchers': 6448,\n",
       " 'today': 6018,\n",
       " 'shop': 5298,\n",
       " '85023': 518,\n",
       " 'savamob': 5114,\n",
       " 'offers': 4304,\n",
       " 'cs': 1597,\n",
       " 'pobox84': 4622,\n",
       " 'm263uz': 3699,\n",
       " '00': 0,\n",
       " 'sub': 5667,\n",
       " '16': 221,\n",
       " 'chance claim': 1257,\n",
       " 'claim ur': 1341,\n",
       " 'ur 150': 6279,\n",
       " '150 worth': 201,\n",
       " 'worth discount': 6787,\n",
       " 'discount vouchers': 1842,\n",
       " 'vouchers today': 6452,\n",
       " 'today text': 6030,\n",
       " 'text shop': 5857,\n",
       " 'shop 85023': 5299,\n",
       " '85023 savamob': 519,\n",
       " 'savamob offers': 5116,\n",
       " 'offers mobile': 4305,\n",
       " 'mobile cs': 3957,\n",
       " 'cs savamob': 1604,\n",
       " 'savamob pobox84': 5117,\n",
       " 'pobox84 m263uz': 4624,\n",
       " 'm263uz 00': 3700,\n",
       " '00 sub': 1,\n",
       " 'sub 16': 5668,\n",
       " 'tmr': 6006,\n",
       " 'bugis': 1092,\n",
       " 'hey tmr': 2828,\n",
       " 'tmr meet': 6008,\n",
       " 'captain': 1188,\n",
       " 'comedy': 1440,\n",
       " 'tv': 6165,\n",
       " 'drunken': 1956,\n",
       " 'grand': 2609,\n",
       " 'later': 3340,\n",
       " 'come later': 1426,\n",
       " 'later tonight': 3346,\n",
       " '10': 138,\n",
       " 'min': 3878,\n",
       " '09061221061': 118,\n",
       " '28days': 293,\n",
       " 'box177': 1030,\n",
       " 'm221bp': 3695,\n",
       " '2yr': 322,\n",
       " 'warranty': 6539,\n",
       " '150ppm': 215,\n",
       " '99': 554,\n",
       " 'camera 09061221061': 1166,\n",
       " '09061221061 landline': 119,\n",
       " 'delivery 28days': 1767,\n",
       " '28days cs': 294,\n",
       " 'cs box177': 1601,\n",
       " 'box177 m221bp': 1031,\n",
       " 'm221bp 2yr': 3696,\n",
       " '2yr warranty': 323,\n",
       " 'warranty 150ppm': 6540,\n",
       " '150ppm 16': 216,\n",
       " '16 99': 222,\n",
       " 'gr8': 2603,\n",
       " 'message': 3852,\n",
       " 'leaving': 3386,\n",
       " 'congrats': 1484,\n",
       " 'school': 5145,\n",
       " 'plans': 4565,\n",
       " 'wat ur': 6551,\n",
       " 'friday': 2357,\n",
       " 'wait': 6471,\n",
       " 'dunno': 1972,\n",
       " 'wot': 6788,\n",
       " 'hell': 2801,\n",
       " 'im': 3000,\n",
       " 'gonna': 2534,\n",
       " 'weeks': 6605,\n",
       " 'im gonna': 3004,\n",
       " 'bring': 1060,\n",
       " 'food': 2271,\n",
       " 'hear': 2784,\n",
       " 'philosophy': 4501,\n",
       " 'say': 5122,\n",
       " 'happen': 2736,\n",
       " 'dont want': 1914,\n",
       " 'want hear': 6515,\n",
       " 'just say': 3198,\n",
       " 'asked': 785,\n",
       " 'afternoon': 639,\n",
       " 'round': 5047,\n",
       " 'til': 5954,\n",
       " 'gt': 2640,\n",
       " 'ish': 3092,\n",
       " 'come round': 1432,\n",
       " 'til like': 5957,\n",
       " 'like lt': 3444,\n",
       " 'lt gt': 3668,\n",
       " 'gt ish': 2653,\n",
       " 'dun': 1962,\n",
       " 'pick': 4528,\n",
       " 'gf': 2469,\n",
       " 'dun need': 1967,\n",
       " 'pick ur': 4533,\n",
       " 'looked': 3562,\n",
       " 'addie': 615,\n",
       " 'monday': 3999,\n",
       " 'sucks': 5681,\n",
       " 'addie goes': 616,\n",
       " 'pls': 4586,\n",
       " 'play': 4566,\n",
       " 'life': 3420,\n",
       " 'pls dont': 4590,\n",
       " 'sir': 5359,\n",
       " 'waiting': 6478,\n",
       " 'sir waiting': 5361,\n",
       " 'depends': 1774,\n",
       " 'individual': 3038,\n",
       " 'hair': 2713,\n",
       " 'dresser': 1941,\n",
       " 'pretty': 4703,\n",
       " 'parents': 4435,\n",
       " 'look': 3560,\n",
       " 'collecting': 1393,\n",
       " 'hair dresser': 2715,\n",
       " 'dunno wat': 1975,\n",
       " 'coming': 1446,\n",
       " 'coming home': 1447,\n",
       " 'saying': 5131,\n",
       " 'order': 4391,\n",
       " 'slippers': 5405,\n",
       " 'cos': 1536,\n",
       " 'pay': 4458,\n",
       " 'saying order': 5133,\n",
       " 'hungry': 2955,\n",
       " 'gay': 2436,\n",
       " 'guys': 2692,\n",
       " '10p': 165,\n",
       " 'texts': 5874,\n",
       " '08712460324': 90,\n",
       " 'just 10p': 3178,\n",
       " '10p min': 166,\n",
       " 'min stop': 3884,\n",
       " 'stop texts': 5634,\n",
       " 'texts 08712460324': 5875,\n",
       " '08712460324 10p': 91,\n",
       " 'accidentally': 585,\n",
       " 'brought': 1075,\n",
       " 'em': 2017,\n",
       " 'bus': 1099,\n",
       " 'aft': 636,\n",
       " 'lect': 3387,\n",
       " 'lar': 3330,\n",
       " 'car': 1189,\n",
       " 'bus stop': 1101,\n",
       " 'aft ur': 638,\n",
       " 'ur lect': 6304,\n",
       " 'lar dun': 3331,\n",
       " 'come pick': 1430,\n",
       " 'tot': 6096,\n",
       " 'group': 2636,\n",
       " 'lucky': 3672,\n",
       " 'havent': 2766,\n",
       " 'leave': 3378,\n",
       " 'tot group': 6097,\n",
       " 'group mate': 2637,\n",
       " 'names': 4117,\n",
       " 'penis': 4473,\n",
       " 'girls': 2488,\n",
       " 'story': 5643,\n",
       " 'doesn': 1862,\n",
       " 'add': 607,\n",
       " 'needs': 4157,\n",
       " 'slowly': 5408,\n",
       " 'vomit': 6442,\n",
       " 'texting': 5869,\n",
       " 'right': 5007,\n",
       " 'ticket': 5949,\n",
       " 'right gonna': 5008,\n",
       " 'sorry': 5490,\n",
       " 'joined': 3153,\n",
       " 'people': 4474,\n",
       " 'touch': 6101,\n",
       " 'mean': 3802,\n",
       " 'times': 5988,\n",
       " 'personal': 4492,\n",
       " 'cost': 1542,\n",
       " 'week': 6584,\n",
       " 'sorry ve': 5505,\n",
       " 'great deal': 2618,\n",
       " 'great week': 2626,\n",
       " 'open': 4365,\n",
       " 'click': 1354,\n",
       " 'make': 3731,\n",
       " 'list': 3480,\n",
       " 'easy': 1992,\n",
       " 'pie': 4543,\n",
       " 'cool': 1522,\n",
       " 'little': 3486,\n",
       " 'getting': 2464,\n",
       " 'soon': 5481,\n",
       " 'getting time': 2465,\n",
       " 'time soon': 5977,\n",
       " 'oops': 4364,\n",
       " 'thk': 5924,\n",
       " 'haf': 2704,\n",
       " 'enuff': 2064,\n",
       " 'thk dun': 5925,\n",
       " 'dun haf': 1965,\n",
       " 'haf enuff': 2705,\n",
       " 'speak': 5529,\n",
       " 'minutes': 3903,\n",
       " 'gt minutes': 2663,\n",
       " 'darren': 1673,\n",
       " 'meeting': 3822,\n",
       " 'ge': 2444,\n",
       " 'den': 1771,\n",
       " 'dinner': 1823,\n",
       " 'xy': 6840,\n",
       " 'feel': 2187,\n",
       " 'lunch': 3677,\n",
       " 'meeting da': 3823,\n",
       " 'da ge': 1649,\n",
       " 'den dun': 1772,\n",
       " 'meet dinner': 3816,\n",
       " 'lunch lor': 3679,\n",
       " 'buying': 1113,\n",
       " 'meh': 3830,\n",
       " 'hi': 2833,\n",
       " 'got money': 2581,\n",
       " 'taunton': 5778,\n",
       " 'll pick': 3517,\n",
       " 'want come': 6510,\n",
       " 'church': 1324,\n",
       " 'holla': 2879,\n",
       " 'things': 5901,\n",
       " 'used': 6354,\n",
       " 'chest': 1301,\n",
       " 'bone': 999,\n",
       " 'april': 745,\n",
       " 'real': 4855,\n",
       " 'date': 1679,\n",
       " 'special': 5532,\n",
       " 'treat': 6118,\n",
       " 'secret': 5174,\n",
       " 'way': 6565,\n",
       " 'wishes': 6699,\n",
       " 'horny': 2917,\n",
       " 'turn': 6162,\n",
       " 'fantasy': 2162,\n",
       " 'hot': 2925,\n",
       " '50': 397,\n",
       " 'cancel': 1180,\n",
       " 'babe im': 854,\n",
       " 'cost 50': 1547,\n",
       " 'cancel send': 1181,\n",
       " 'send stop': 5221,\n",
       " 'tel': 5807,\n",
       " 'software': 5459,\n",
       " 'bb': 885,\n",
       " 'wont': 6749,\n",
       " 'use': 6350,\n",
       " 'wife': 6649,\n",
       " 'doctor': 1859,\n",
       " 'madam': 3712,\n",
       " 'happened': 2738,\n",
       " 'interview': 3066,\n",
       " 'imma': 3012,\n",
       " 'cause': 1237,\n",
       " 'jay': 3129,\n",
       " 'wants': 6533,\n",
       " 'drugs': 1955,\n",
       " 'messages': 3861,\n",
       " 'reply stop': 4961,\n",
       " 'customer services': 1635,\n",
       " 'ask': 778,\n",
       " 'macho': 3710,\n",
       " 'budget': 1090,\n",
       " 'bold': 997,\n",
       " 'saw': 5120,\n",
       " 'dollars': 1879,\n",
       " 'ask macho': 781,\n",
       " 'gt dollars': 2648,\n",
       " 'said': 5085,\n",
       " 'said text': 5091,\n",
       " 'mr': 4048,\n",
       " 'foley': 2262,\n",
       " 'won': 6731,\n",
       " 'ipod': 3083,\n",
       " 'prizes': 4727,\n",
       " 'eye': 2135,\n",
       " 'visit': 6428,\n",
       " '82050': 499,\n",
       " 'win winner': 6676,\n",
       " 'winner mr': 6682,\n",
       " 'mr foley': 4049,\n",
       " 'foley won': 2263,\n",
       " 'won ipod': 6740,\n",
       " 'ipod exciting': 3085,\n",
       " 'exciting prizes': 2107,\n",
       " 'prizes soon': 4728,\n",
       " 'soon eye': 5483,\n",
       " 'eye ur': 2136,\n",
       " 'ur mobile': 6314,\n",
       " 'mobile visit': 3968,\n",
       " 'visit www': 6430,\n",
       " 'www win': 6824,\n",
       " 'win 82050': 6667,\n",
       " '82050 uk': 500,\n",
       " 'jesus': 3139,\n",
       " 'christ': 1322,\n",
       " 'bitch': 968,\n",
       " 'answer': 715,\n",
       " 'fucking': 2395,\n",
       " 'trouble': 6131,\n",
       " 'stranger': 5648,\n",
       " 'dave': 1684,\n",
       " 'sorted': 5507,\n",
       " 'bloke': 980,\n",
       " 'gona': 2531,\n",
       " 'mum': 4087,\n",
       " 'thinks': 5922,\n",
       " 'tessy': 5840,\n",
       " 'favor': 2179,\n",
       " 'convey': 1515,\n",
       " 'birthday': 959,\n",
       " 'nimya': 4222,\n",
       " 'dnt': 1854,\n",
       " 'forget': 2283,\n",
       " 'shijas': 5289,\n",
       " 'tessy pls': 5841,\n",
       " 'pls favor': 4591,\n",
       " 'favor pls': 2180,\n",
       " 'pls convey': 4588,\n",
       " 'convey birthday': 1516,\n",
       " 'birthday wishes': 963,\n",
       " 'wishes nimya': 6700,\n",
       " 'nimya pls': 4223,\n",
       " 'pls dnt': 4589,\n",
       " 'dnt forget': 1855,\n",
       " 'forget today': 2284,\n",
       " 'today birthday': 6020,\n",
       " 'birthday shijas': 962,\n",
       " 'unique': 6247,\n",
       " 'august': 812,\n",
       " 'works': 6771,\n",
       " 'years': 6874,\n",
       " 'doesnt': 1864,\n",
       " 'bother': 1021,\n",
       " 'gt years': 2671,\n",
       " 'years old': 6875,\n",
       " 'address': 617,\n",
       " 'test': 5842,\n",
       " 'considering': 1493,\n",
       " 'computer': 1469,\n",
       " 'thts': 5943,\n",
       " 'god': 2505,\n",
       " 'gift': 2472,\n",
       " 'birds': 955,\n",
       " 'natural': 4131,\n",
       " 'frm': 2379,\n",
       " 'reverse': 4996,\n",
       " 'cheating': 1289,\n",
       " 'mathematics': 3784,\n",
       " 'reverse cheating': 4997,\n",
       " 'cheating mathematics': 1290,\n",
       " 'marry': 3771,\n",
       " 'lovers': 3646,\n",
       " 'problems': 4735,\n",
       " 'dis': 1831,\n",
       " 'wil': 6657,\n",
       " 'news': 4200,\n",
       " 'person': 4483,\n",
       " 'tomorrow': 6057,\n",
       " 'best': 926,\n",
       " 'break': 1054,\n",
       " 'chain': 1254,\n",
       " 'frnds': 2384,\n",
       " 'mins': 3893,\n",
       " 'whn': 6637,\n",
       " 'read': 4845,\n",
       " 'person like': 4488,\n",
       " 'like tomorrow': 3454,\n",
       " 'send lt': 5212,\n",
       " 'gt mins': 2662,\n",
       " 'difficult': 1814,\n",
       " 'simple': 5344,\n",
       " 'enter': 2049,\n",
       " 'elaine': 2014,\n",
       " 'confirmed': 1480,\n",
       " 'elaine today': 2015,\n",
       " 'today meeting': 6025,\n",
       " 'ela': 2013,\n",
       " 'normal': 4257,\n",
       " 'pleased': 4579,\n",
       " 'means': 3806,\n",
       " 'february': 2186,\n",
       " 'stay': 5599,\n",
       " 'audition': 809,\n",
       " 'season': 5171,\n",
       " 'sister': 5364,\n",
       " 'moved': 4038,\n",
       " 'away': 842,\n",
       " 'audition season': 810,\n",
       " 'theory': 5898,\n",
       " 'going': 2515,\n",
       " 'book': 1004,\n",
       " '21': 270,\n",
       " 'coz': 1567,\n",
       " 'wanna': 6503,\n",
       " 'jiayin': 3140,\n",
       " 'isnt': 3097,\n",
       " 'head': 2779,\n",
       " 'usf': 6357,\n",
       " 'like minutes': 3445,\n",
       " 'texted': 5868,\n",
       " 'finished': 2236,\n",
       " 'long': 3554,\n",
       " 'ago': 651,\n",
       " 'er': 2070,\n",
       " 'long time': 3558,\n",
       " 'freemsg': 2347,\n",
       " 'baby': 859,\n",
       " 'wow': 6791,\n",
       " 'cam': 1157,\n",
       " 'moby': 3985,\n",
       " 'pic': 4526,\n",
       " 'fancy': 2156,\n",
       " 'rply': 5052,\n",
       " '82242': 501,\n",
       " 'hlp': 2857,\n",
       " '08712317606': 82,\n",
       " 'msg150p': 4068,\n",
       " '2rcv': 314,\n",
       " 'hi baby': 2837,\n",
       " 'just got': 3191,\n",
       " 'got new': 2582,\n",
       " 'cam moby': 1158,\n",
       " 'moby wanna': 3987,\n",
       " 'hlp 08712317606': 2858,\n",
       " 'msg150p 2rcv': 4069,\n",
       " 'practice': 4687,\n",
       " 'smart': 5415,\n",
       " '200': 254,\n",
       " 'weekly': 6600,\n",
       " 'quiz': 4791,\n",
       " 'po': 4611,\n",
       " 'm26': 3697,\n",
       " '3uz': 369,\n",
       " 'think ur': 5918,\n",
       " 'win 200': 6664,\n",
       " 'po box': 4612,\n",
       " 'm26 3uz': 3698,\n",
       " '50 week': 406,\n",
       " 'anthony': 720,\n",
       " 'bringing': 1061,\n",
       " 'fees': 2201,\n",
       " 'rent': 4937,\n",
       " 'stuff': 5659,\n",
       " 'thats': 5889,\n",
       " 'help': 2811,\n",
       " 'pay rent': 4460,\n",
       " 'stuff like': 5660,\n",
       " 'need help': 4150,\n",
       " 'friend need': 2362,\n",
       " 'points': 4628,\n",
       " 'module': 3991,\n",
       " 'missing': 3921,\n",
       " 'plenty': 4584,\n",
       " 'just finished': 3190,\n",
       " 'pub': 4758,\n",
       " 'later pls': 3345,\n",
       " 'tone': 6065,\n",
       " 'mob': 3940,\n",
       " 'nok': 4230,\n",
       " '87021': 525,\n",
       " '1st': 241,\n",
       " 'txtin': 6201,\n",
       " 'friends': 2364,\n",
       " 'hl': 2854,\n",
       " '4info': 389,\n",
       " 'nokia tone': 4243,\n",
       " 'tone ur': 6075,\n",
       " 'ur mob': 6313,\n",
       " 'mob week': 3944,\n",
       " 'week just': 6590,\n",
       " 'just txt': 3210,\n",
       " 'txt nok': 6191,\n",
       " 'nok 87021': 4231,\n",
       " '87021 1st': 526,\n",
       " '1st tone': 243,\n",
       " 'tone free': 6071,\n",
       " 'free txtin': 2341,\n",
       " 'txtin tell': 6202,\n",
       " 'tell ur': 5818,\n",
       " 'ur friends': 6297,\n",
       " 'friends 150p': 2365,\n",
       " '150p tone': 210,\n",
       " 'tone 16': 6066,\n",
       " '16 reply': 226,\n",
       " 'reply hl': 4954,\n",
       " 'hl 4info': 2855,\n",
       " 'died': 1811,\n",
       " 'didn': 1804,\n",
       " 'family': 2154,\n",
       " 'str': 5645,\n",
       " 'orchard': 4389,\n",
       " 'going lunch': 2523,\n",
       " 'lunch wif': 3683,\n",
       " 'aft dat': 637,\n",
       " 'request': 4971,\n",
       " 'set': 5255,\n",
       " 'callertune': 1144,\n",
       " 'callers': 1142,\n",
       " 'press': 4699,\n",
       " 'copy': 1529,\n",
       " 'set callertune': 5256,\n",
       " 'callertune callers': 1145,\n",
       " 'callers press': 1143,\n",
       " 'press copy': 4700,\n",
       " 'copy friends': 1530,\n",
       " 'friends callertune': 2368,\n",
       " '0776xxxxxxx': 31,\n",
       " 'invited': 3072,\n",
       " 'xchat': 6828,\n",
       " 'final': 2223,\n",
       " 'attempt': 802,\n",
       " 'msgrcvdhg': 4072,\n",
       " 'suite342': 5686,\n",
       " '2lands': 298,\n",
       " 'row': 5048,\n",
       " 'w1j6hl': 6462,\n",
       " 'ldn': 3368,\n",
       " '18yrs': 237,\n",
       " 'dear 0776xxxxxxx': 1720,\n",
       " '0776xxxxxxx ve': 32,\n",
       " 've invited': 6390,\n",
       " 'invited xchat': 3074,\n",
       " 'xchat final': 6829,\n",
       " 'final attempt': 2224,\n",
       " 'attempt contact': 803,\n",
       " 'contact txt': 1505,\n",
       " '150p msgrcvdhg': 208,\n",
       " 'msgrcvdhg suite342': 4073,\n",
       " 'suite342 2lands': 5687,\n",
       " '2lands row': 299,\n",
       " 'row w1j6hl': 5049,\n",
       " 'w1j6hl ldn': 6463,\n",
       " 'ldn 18yrs': 3370,\n",
       " 'lousy': 3602,\n",
       " 'quit': 4787,\n",
       " 'lei': 3399,\n",
       " 'shd': 5282,\n",
       " 'sch': 5142,\n",
       " 'hr': 2942,\n",
       " 'oni': 4360,\n",
       " 'dunno lei': 1974,\n",
       " 'lor cos': 3572,\n",
       " 'ah': 653,\n",
       " 'confuses': 1482,\n",
       " 'maybe': 3798,\n",
       " 'wrong': 6800,\n",
       " 'thing': 5900,\n",
       " 'sort': 5506,\n",
       " 'tho': 5931,\n",
       " 'ah confuses': 654,\n",
       " 'confuses things': 1483,\n",
       " 'called': 1135,\n",
       " 'dad': 1660,\n",
       " 'oredi': 4396,\n",
       " 'boy': 1039,\n",
       " 'father': 2170,\n",
       " 'power': 4678,\n",
       " 'frndship': 2387,\n",
       " 'boy late': 1040,\n",
       " 'late home': 3337,\n",
       " 'home father': 2888,\n",
       " ...}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00 sub', '000', '000 bonus', '000 cash', '000 homeowners', '000 pounds', '000 prize', '008704050406', '008704050406 sp', '02', '02 06', '0207', '0207 153', '02073162414', '02073162414 costs', '03', '03 05', '03 2nd', '04', '05', '050703', '050703 csbcm4235wc1n3xx', '06', '06 03', '07', '07 11', '07123456789', '07123456789 87077', '07734396839', '07734396839 ibh', '0776xxxxxxx', '0776xxxxxxx ve', '07781482378', '07781482378 com', '07xxxxxxxxx', '07xxxxxxxxx won', '08', '0800', '0800 1956669', '0800 542', '08000407165', '08000407165 18', '08000776320', '08000776320 reply', '08000839402', '08000839402 2stoptxt', '08000839402 call2optout', '08000930705', '08000930705 delivery']\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = vect.get_feature_names()\n",
    "print(X_train_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year best', 'year old', 'year special', 'year supply', 'years', 'years old', 'years shower', 'yeh', 'yelling', 'yellow', 'yep', 'yer', 'yer mobile', 'yes', 'yes 440', 'yes 85023', 'yes callback', 'yes knw', 'yes princess', 'yes really', 'yes speak', 'yes tv', 'yest', 'yesterday', 'yesterday today', 'yetunde', 'yijue', 'ym', 'yo', 'yo yo', 'yoga', 'yogasana', 'yor', 'youre', 'yr', 'yr prize', 'yrs', 'yummy', 'yun', 'yun ah', 'yuo', 'yuo exmpel', 'yuo raed', 'yup', 'yup ok', 'yup thk', 'zed', 'zed 08701417012', 'zed 08701417012150p', 'zoe']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 7.,  4., 22., ...,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature count per class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "ham_token_count = nb.feature_count_[0, :]\n",
    "\n",
    "# number of times each token appears across all SPAM messages\n",
    "spam_token_count = nb.feature_count_[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 sub</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 bonus</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 cash</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "00         0.0   7.0\n",
       "00 sub     0.0   4.0\n",
       "000        0.0  22.0\n",
       "000 bonus  0.0   6.0\n",
       "000 cash   0.0   7.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, 'spam':spam_token_count}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summer free</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x150p</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nw came</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200 free</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ham  spam\n",
       "token                 \n",
       "summer free  0.0   2.0\n",
       "1x150p       0.0   4.0\n",
       "nw came      2.0   0.0\n",
       "shock        1.0   1.0\n",
       "200 free     0.0   2.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes counts the number of observations in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3618.,  561.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 to ham and spam counts to avoid dividing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summer free</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x150p</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nw came</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200 free</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ham  spam\n",
       "token                 \n",
       "summer free  1.0   3.0\n",
       "1x150p       1.0   5.0\n",
       "nw came      3.0   1.0\n",
       "shock        2.0   2.0\n",
       "200 free     1.0   3.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summer free</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.008913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nw came</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200 free</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ham      spam\n",
       "token                          \n",
       "summer free  0.000276  0.005348\n",
       "1x150p       0.000276  0.008913\n",
       "nw came      0.000829  0.001783\n",
       "shock        0.000553  0.003565\n",
       "200 free     0.000276  0.005348"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ratio of spam-to-ham for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summer free</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>19.347594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>32.245989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nw came</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>2.149733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>6.449198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200 free</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>19.347594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ham      spam  spam_ratio\n",
       "token                                      \n",
       "summer free  0.000276  0.005348   19.347594\n",
       "1x150p       0.000276  0.008913   32.245989\n",
       "nw came      0.000829  0.001783    2.149733\n",
       "shock        0.000553  0.003565    6.449198\n",
       "200 free     0.000276  0.005348   19.347594"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the DataFrame sorted by spam_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>509.486631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>380.502674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>348.256684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.083779</td>\n",
       "      <td>303.112299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.071301</td>\n",
       "      <td>257.967914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.057582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.031509</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.056572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.032338</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.055121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.065782</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.027097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.066059</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.026984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7231 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ham      spam  spam_ratio\n",
       "token                                \n",
       "claim  0.000276  0.140820  509.486631\n",
       "prize  0.000276  0.105169  380.502674\n",
       "150p   0.000276  0.096257  348.256684\n",
       "tone   0.000276  0.083779  303.112299\n",
       "18     0.000276  0.071301  257.967914\n",
       "...         ...       ...         ...\n",
       "later  0.030956  0.001783    0.057582\n",
       "lor    0.031509  0.001783    0.056572\n",
       "da     0.032338  0.001783    0.055121\n",
       "lt     0.065782  0.001783    0.027097\n",
       "gt     0.066059  0.001783    0.026984\n",
       "\n",
       "[7231 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.593582887700535"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.loc['00', 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the vectorizer\n",
    "Do you see any potential to enhance the vectorizer? Think about the following questions:  \n",
    "__Are all word equally important?__  \n",
    "__Do you think there are \"noise words\" which negatively influence the results?__  \n",
    "__How can we account for the order of words?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "Stopwords are the most common words in a language. Examples are 'is', 'which' and 'the'. Usually is beneficial to exclude these words in text processing tasks.  \n",
    "The `CountVectorizer` has a `stop_words` parameter:\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams concatenate n words to form a token. The following accounts for 1- and 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it's beneficial to exclude words that appear in the majority or just a couple of documents. This is, very frequent or infrequent words. This can be achieved by using the `max_df` and `min_df` parameters of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)\n",
    "\n",
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on Stemming\n",
    "* 'went' and 'go'  \n",
    "* 'kids' and 'kid'  \n",
    "* 'negative' and 'negatively'\n",
    "\n",
    "__What is the pattern?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing a word to it's word stem, base or root form is called _stemming_. Scikit-Learn has no powerfull stemmer, but other libraries like the [NLTK](http://www.nltk.org/) have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf\n",
    "* Tf-idf can be understood as a modification of the *raw term frequencies* (tf)\n",
    "* The concept behind tf-idf is to downweight terms proportionally to the number of documents in which they occur.\n",
    "* The idea is that terms that occur in many different documents are likely unimportant or don't contain any useful information for Natural Language Processing tasks such as document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explanation by example\n",
    "Let consider a dataset containing 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compute the _term frequency_ (alternatively: Bag-of-Words) $tf(t, d)$. $t$ is the number of times a term occures in a document $d$. Using Scikit-Learn we can quickly get those numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = cv.fit_transform(docs).toarray()\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we introduce *inverse document frequency* ($idf$) by defining the term *document frequency* $\\text{df}(d,t)$, which is simply the number of documents $d$ that contain the term $t$. We can then define the idf as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{n_d}{1+\\text{df}(d,t)}},$$ \n",
    "where  \n",
    "$n_d$: The total number of documents  \n",
    "$\\text{df}(d,t)$: The number of documents that contain term $t$.\n",
    "\n",
    "Note that the constant 1 is added to the denominator to avoid a zero-division error if a term is not contained in any document in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let us calculate the idfs of the words \"and\", \"is,\" and \"shining:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf \"and\": 0.4054651081081644\n",
      "idf \"is\": -0.2876820724517809\n",
      "idf \"shining\": 0.0\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(docs)\n",
    "\n",
    "df_and = 1\n",
    "idf_and = np.log(n_docs / (1 + df_and))\n",
    "print('idf \"and\": %s' % idf_and)\n",
    "\n",
    "df_is = 3\n",
    "idf_is = np.log(n_docs / (1 + df_is))\n",
    "print('idf \"is\": %s' % idf_is)\n",
    "\n",
    "df_shining = 2\n",
    "idf_shining = np.log(n_docs / (1 + df_shining))\n",
    "print('idf \"shining\": %s' % idf_shining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those idfs, we can eventually calculate the tf-idfs for the 3rd document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t),$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idfs in document 3:\n",
      "\n",
      "tf-idf \"and\": 0.4054651081081644\n",
      "tf-idf \"is\": -0.5753641449035618\n",
      "tf-idf \"shining\": 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Tf-idfs in document 3:\\n')\n",
    "print('tf-idf \"and\": %s' % (1 * idf_and))\n",
    "print('tf-idf \"is\": %s' % (2 * idf_is))\n",
    "print('tf-idf \"shining\": %s' % (1 * idf_shining))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.        , 1.40546511])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wait! Those numbers aren't the same!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf in Scikit-Learn is calculated a little bit differently. Here, the `+1` count is added to the idf, whereas instead of the denominator if the df:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{n_d}{\\text{df}(d,t)}} + 1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09861228866811"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_and = 1\n",
    "df_and = 1 \n",
    "tf_and * (np.log(n_docs / df_and) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_is = 2\n",
    "df_is = 3 \n",
    "tf_is * (np.log(n_docs / df_is) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_shining = 1\n",
    "df_shining = 2 \n",
    "tf_shining * (np.log(n_docs / df_shining) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "By default, Scikit-Learn performs a normalization. The most common way to normalize the raw term frequency is l2-normalization, i.e., dividing the raw term frequency vector $v$ by its length $||v||_2$ (L2- or Euclidean norm).\n",
    "\n",
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}$$\n",
    "\n",
    "__Why is that useful?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46572049, 0.44383662, 0.31189844])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not quite there. Sckit-Learn also applies smoothing, which changes the original formula as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40474829, 0.47810172, 0.30782151])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm='l2')\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
